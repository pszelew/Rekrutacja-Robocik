from __future__ import annotations
from layers.layer import Layer
from layers.conv import Conv
from layers.dense import Dense
from layers.flatten import Flatten
from layers.max_pooling import MaxPooling
import numpy as np

# Load numpy library

print("Loading my_network_network")
print(f"Numpy version {np.__version__}")
# Print image to know that we loaded our network package

# Proposed network structure
# 1) conv2d(3x3)
# 2) max_polling(2x2)
# 3) conv2d(3x3)
# 4) max_polling(2x2)
# 5) conv2d(3x3)
# 6) flatten
# 7) dense(ReLU)
# 8) dense(softmax)


class MyConvNetwork:
    """
    A class used to represent a convolutional network used to solve MNIST detection problem

    Attributes
    ----------
    batch_size: int
        Batch size
    layers: list[Layer]
        List of layers
    weights: list[np.array]
        List keeping weights used by consecutive layers of network
    states: list[np.array]
        List containing consecutive values generated by neurons
    Methods
    -------
    relu(val: np.float64) -> np.float64
        ReLU function
    softmax(arr: np.array) -> np.array:
        softmax function
    max_polling(arr: np.array, n: int) -> np.array
        max-polling operation
    flatten(arr: np.array) -> np.array
        Operation flattening given array to 1D array
    conv2d_forward(input: np.array, weights: np.array, bias: np.array) -> np.array:
        max-polling operation
    """
    def __init__(self, batch_size: int = 10, lr: float = 0.001):
        self.lr: float
        self.lr = lr
        self.weights: list
        self.weights = []
        self.weights.append(np.random.rand(3, 3, 1, 32))
        # Weights for 1st conv layer
        self.weights.append(np.random.rand(3, 3, 32, 64))
        # Weights for 2nd conv layer
        self.weights.append(np.random.rand(64, 576))
        # Weights for 1nd dense layer
        self.weights.append(np.random.rand(10, 64))
        # Weights for 2nd dense layer

        self.bias: list
        self.bias = []
        self.bias.append(np.random.rand(32))
        # Bias for 1st conv layer
        self.bias.append(np.random.rand(64))
        # Bias for 2nd conv layer
        self.bias.append(np.random.rand(576))
        # Bias for 1st dense layer
        self.bias.append(np.random.rand(64))
        # Bias for 2nd dense layer

        self.layers: list
        self.layers = []
        self.batch_size: int
        self.batch_size = batch_size
        self.layers.append(Conv((batch_size, 28, 28, 1), (batch_size, 26, 26, 32)))
        # First conv layer
        self.layers.append(MaxPooling((batch_size, 26, 26, 32), (batch_size, 13, 13, 32)))
        # Max pooling layer to reduce size
        self.layers.append(Conv((batch_size, 13, 13, 32), (batch_size, 11, 11, 64)))
        # Second conv layer
        self.layers.append(MaxPooling((batch_size, 11, 11, 64), (batch_size, 5, 5, 64)))
        # Max pooling layer to reduce size
        self.layers.append(Conv((batch_size, 5, 5, 64), (batch_size, 3, 3, 64)))
        # Third conv layer
        self.layers.append(Flatten((batch_size, 3, 3, 64), (batch_size, 576)))
        # Flatten layer to prepare data for Dense layers
        self.layers.append(Dense(self.relu, (batch_size, 576), (batch_size, 64)))
        # First dense layer
        self.layers.append(Dense(self.softmax, (batch_size, 64), (batch_size, 10)))
        # Second dense layer. Output of network

    def relu(self, arr: np.array) -> np.array:
        """ReLU function

        Parameters
        ----------
        arr: np.array
            Input array of ReLU function

        Returns
        -------
        np.float64
            Result of ReLU function
        """
        out_arr = np.zeros_like(arr)
        # Output array
        for i, item in enumerate(arr):
            if item > 0:
                out_arr[i] = item
            else:
                out_arr[i] = np.float64(0)
        return out_arr

    def softmax(self, arr: np.array) -> np.array:
        """softmax function

        Parameters
        ----------
        arr: np.array
            Input array of softmax function

        Returns
        -------
        np.array
            Result of softmax function
        """
        temp_lst: list
        temp_lst = [] 
        exp_sum: np.float64
        exp_sum = np.sum([np.exp(item) for item in arr])
        #calculate sum(e^zi)
        for item in arr:
            # For every value in our given array
            temp_val: np.float64
            temp_val = (np.exp(item))/(exp_sum)
            temp_lst.append(temp_val)
        return np.array(temp_lst)


    def forward(self, input_data: np.array) -> np.array:
        """Forward pass function

        Parameters
        ----------
        input_data: np.array
            Input array of our network

        Returns
        -------
        np.array
            Prediction of network
        """
        count_weights: int
        count_weights = 0
        temp_data: np.array
        temp_data = input_data
        for layer in self.layers:
            if layer.name is "conv":
                temp_data = layer.forward(temp_data, self.weights[count_weights], self.bias[count_weights])
                count_weights += 1
            elif layer.name is "dense":
                temp_data = layer.forward(temp_data, self.weights[count_weights], self.bias[count_weights])
                count_weights += 1
            elif layer.name is "flatten":
                temp_data = layer.forward(temp_data)
            elif layer.name is "max_pooling":
                temp_data = layer.forward(temp_data)
            else:
                print("Layer not found!")
                exit()
        return temp_data
        
    def backward(self, train_labels):
        """Backward pass of our network

        Update weights 

        Parameters
        ----------
        input_data: np.array
            Input array of our network

        Returns
        -------
        np.array
            Prediction of network
        """

        # for i in range(self.layers.count):
            

if __name__ == "__main__":
    my_conv = MyConvNetwork()
    # To test our defined functions
    test_conv = np.array([[
        [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]],
        [[2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8]],
        [[3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]],
        [[4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10]],
        [[5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]],
        [[6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12]]]]
    )
    weights_conv = np.random.rand(3, 3, 2, 32)
    bias_conv = np.random.rand(32)
    # print(test_pooling.shape)
    # print(type(my_conv.relu(np.float64(1.3))))
    # print(my_conv.softmax(np.array([15, 0.4, 1])))
    # print((my_conv.max_pooling(test_pooling, 2)).shape)
    print("Conv:")
    #print(my_conv.conv2d_forward(test_conv, weights_conv, bias_conv))
    
    test_dense = np.array([[1, 2, 3],[4, 5, 6]])
    weights_dense = np.random.rand(2, 3)
    # outputs, inputs
    bias_dense = np.random.rand(2)
    print("Dense:")
    #print(my_conv.dense(my_conv.relu, test_dense, weights_dense, bias_dense))
